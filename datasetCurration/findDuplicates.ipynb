{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8UYMKKKob-m"
   },
   "source": [
    "# Detect duplicate images\n",
    "\n",
    "## 1. Compile and transfere your images from your local computer to google drive\n",
    "\n",
    "1. On your local computer make a folder `images`.\n",
    "2. Put all images from one insect type in the folder `images`.\n",
    "3. Compile the folder to `images.zip`. On mac use `ditto -c -k --norsrc --keepParent images images.zip` to exclude finderfiles from the zipped file.\n",
    "4. Put the zipped images in you google drive\n",
    "5. Continue with the next code blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABFb-xklSl-N"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Unzip file with images\n",
    "!mkdir '/content/images'\n",
    "!scp '/content/gdrive/MyDrive/images.zip' '/content/images.zip'\n",
    "!unzip '/content/images.zip'\n",
    "\n",
    "# Optional: Check is path is correct\n",
    "!ls '/content/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljmPzr3fM8Uk"
   },
   "outputs": [],
   "source": [
    "!pip install ImageHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCl6_SWrc6bx"
   },
   "source": [
    "## 2. Vergelijken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nmwIP1BgCGi"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor  # For parallel processing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash  # For perceptual hashing of images\n",
    "from tqdm import tqdm  # For progress visualization\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow  # For displaying images in Google Colab\n",
    "\n",
    "# Define output folder and file for saving results\n",
    "output_folder = '/content/images'  # Directory where images are stored\n",
    "output_file = 'similar_images.txt'  # File to store results of similar image pairs\n",
    "\n",
    "# Function to compute perceptual hashes for an image\n",
    "def compute_image_hash(image_path, hash_sizes=[8, 16, 32]):\n",
    "    try:\n",
    "        # Open image file and convert to grayscale\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"L\")\n",
    "            hashes = []\n",
    "            # Compute hashes for each specified size\n",
    "            for size in hash_sizes:\n",
    "                resized = img.resize((size * 4, size * 4), Image.LANCZOS)\n",
    "                hashes.append(str(imagehash.phash(resized, hash_size=size)))\n",
    "            return hashes\n",
    "    except Exception:\n",
    "        # Handle errors during hash computation\n",
    "        return None\n",
    "\n",
    "# Function to precompute hashes for all images\n",
    "def precompute_hashes(image_files):\n",
    "    hashes = {}\n",
    "    # Compute hash for each image\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(output_folder, img_file)\n",
    "        img_hash = compute_image_hash(img_path)\n",
    "        if img_hash:\n",
    "            hashes[img_file] = img_hash\n",
    "    return hashes\n",
    "\n",
    "# Function to compare hashes in a given chunk of data\n",
    "def compare_hashes_chunk(chunk):\n",
    "    similar_pairs = []\n",
    "    # Iterate through pairs of images in the chunk\n",
    "    for i in range(len(chunk)):\n",
    "        for j in range(i + 1, len(chunk)):\n",
    "            img1, hashes1 = chunk[i]\n",
    "            img2, hashes2 = chunk[j]\n",
    "            if hashes1 is not None and hashes2 is not None:\n",
    "                # Calculate similarity for each hash size\n",
    "                similarities = [1 - (int(h1, 16) ^ int(h2, 16)).bit_count() / (len(h1) * 4)\n",
    "                                for h1, h2 in zip(hashes1, hashes2)]\n",
    "                max_similarity = max(similarities)\n",
    "                # Check if similarity exceeds threshold\n",
    "                if max_similarity > 0.80:  # Threshold for similarity\n",
    "                    similar_pairs.append((max_similarity, (img1, img2)))\n",
    "    return similar_pairs\n",
    "\n",
    "# Function to find all similar image pairs based on precomputed hashes\n",
    "def find_similar_pairs(hashes):\n",
    "    hash_items = list(hashes.items())\n",
    "    # Divide data into chunks for parallel processing\n",
    "    chunk_size = max(1, len(hash_items) // os.cpu_count())\n",
    "    chunks = [hash_items[i:i + chunk_size] for i in range(0, len(hash_items), chunk_size)]\n",
    "\n",
    "    # Use ProcessPoolExecutor to parallelize hash comparisons\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(compare_hashes_chunk, chunks), total=len(chunks), desc=\"Comparing hashes\"))\n",
    "\n",
    "    # Flatten results from all chunks\n",
    "    return [item for sublist in results for item in sublist]\n",
    "\n",
    "# Function to display an image with its filename using OpenCV\n",
    "def display_image_with_filename(image_path):\n",
    "    try:\n",
    "        # Load and resize the image for display\n",
    "        img = cv2.imread(image_path)\n",
    "        img_resized = cv2.resize(img, (64, 64))\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        # Create an overlay with the filename\n",
    "        text_img = np.zeros((20, 64, 3), dtype=np.uint8)\n",
    "        cv2.putText(text_img, filename, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "\n",
    "        # Combine image and overlay\n",
    "        combined_img = np.vstack((img_resized, text_img))\n",
    "\n",
    "        # Display the image in Colab\n",
    "        cv2_imshow(combined_img)\n",
    "        print(f\"Filename: {filename}\")\n",
    "    except Exception as e:\n",
    "        # Handle errors during image display\n",
    "        print(f\"Error displaying image {image_path}: {e}\")\n",
    "\n",
    "# Function to process a list of images and find similar pairs\n",
    "def process_images(image_list):\n",
    "    start_time = time.time()  # Record start time\n",
    "\n",
    "    # Precompute hashes for all images\n",
    "    print(\"Precomputing hashes...\")\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        chunk_size = max(1, len(image_list) // os.cpu_count())\n",
    "        chunks = [image_list[i:i + chunk_size] for i in range(0, len(image_list), chunk_size)]\n",
    "        results = list(tqdm(executor.map(precompute_hashes, chunks), total=len(chunks), desc=\"Computing hashes\"))\n",
    "\n",
    "    # Combine results into a single dictionary\n",
    "    image_hashes = {k: v for d in results for k, v in d.items()}\n",
    "\n",
    "    # Find pairs of similar images\n",
    "    print(\"Finding similar pairs...\")\n",
    "    similar_pairs = find_similar_pairs(image_hashes)\n",
    "\n",
    "    # Sort similar pairs by similarity score\n",
    "    similar_pairs.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Display and save similar pairs\n",
    "    print(\"\\nSimilar pairs above 0.85 threshold:\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i, (similarity, (img1_name, img2_name)) in enumerate(similar_pairs):\n",
    "            print(f\"\\nPair {i+1} with similarity: {similarity:.4f}\")\n",
    "            f.write(f\"Pair {i+1} with similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "            img1_path = os.path.join(output_folder, img1_name)\n",
    "            img2_path = os.path.join(output_folder, img2_name)\n",
    "\n",
    "            print(\"Image 1:\")\n",
    "            print(f\"Filename: {img1_name}\")\n",
    "            f.write(f\"Image 1: {img1_name}\\n\")\n",
    "            display_image_with_filename(img1_path)\n",
    "\n",
    "            print(\"\\nImage 2:\")\n",
    "            print(f\"Filename: {img2_name}\")\n",
    "            f.write(f\"Image 2: {img2_name}\\n\\n\")\n",
    "            display_image_with_filename(img2_path)\n",
    "\n",
    "    # Display summary and execution time\n",
    "    print(f\"\\nTotal similar pairs found: {len(similar_pairs)}\")\n",
    "    print(f\"Results have been saved to {output_file}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Comparison complete! Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Main function to initiate image processing\n",
    "def main():\n",
    "    print(\"Processing all images in the folder...\")\n",
    "    all_images = os.listdir(output_folder)  # Get list of all images in the output folder\n",
    "    process_images(all_images)  # Process images for similarity\n",
    "\n",
    "# Run the main function if the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ypmHq0SHJT6t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Output folder path (change as needed)\n",
    "output_folder = '/content/images'\n",
    "output_file = 'similar_images.txt'\n",
    "\n",
    "def compute_image_hash(image_path, hash_sizes=[8, 16, 32]):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"L\")\n",
    "            hashes = []\n",
    "            for size in hash_sizes:\n",
    "                resized = img.resize((size * 4, size * 4), Image.LANCZOS)\n",
    "                hashes.append(str(imagehash.phash(resized, hash_size=size)))\n",
    "            return hashes\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def precompute_hashes(image_files):\n",
    "    hashes = {}\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(output_folder, img_file)\n",
    "        img_hash = compute_image_hash(img_path)\n",
    "        if img_hash:\n",
    "            hashes[img_file] = img_hash\n",
    "    return hashes\n",
    "\n",
    "def compare_hashes_chunk(chunk):\n",
    "    similar_pairs = []\n",
    "    for i in range(len(chunk)):\n",
    "        for j in range(i + 1, len(chunk)):\n",
    "            img1, hashes1 = chunk[i]\n",
    "            img2, hashes2 = chunk[j]\n",
    "            if hashes1 is not None and hashes2 is not None:\n",
    "                similarities = [1 - (int(h1, 16) ^ int(h2, 16)).bit_count() / (len(h1) * 4)\n",
    "                                for h1, h2 in zip(hashes1, hashes2)]\n",
    "                max_similarity = max(similarities)\n",
    "                if max_similarity > 0.80:  # Threshold set to 0.85\n",
    "                    similar_pairs.append((max_similarity, (img1, img2)))\n",
    "    return similar_pairs\n",
    "\n",
    "def find_similar_pairs(hashes):\n",
    "    hash_items = list(hashes.items())\n",
    "    chunk_size = max(1, len(hash_items) // os.cpu_count())\n",
    "    chunks = [hash_items[i:i + chunk_size] for i in range(0, len(hash_items), chunk_size)]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(compare_hashes_chunk, chunks), total=len(chunks), desc=\"Comparing hashes\"))\n",
    "\n",
    "    return [item for sublist in results for item in sublist]\n",
    "\n",
    "def display_image_with_filename(image_path):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img_resized = cv2.resize(img, (64, 64))\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        text_img = np.zeros((20, 64, 3), dtype=np.uint8)\n",
    "        cv2.putText(text_img, filename, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "\n",
    "        combined_img = np.vstack((img_resized, text_img))\n",
    "\n",
    "        cv2_imshow(combined_img)\n",
    "        print(f\"Filename: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image {image_path}: {e}\")\n",
    "\n",
    "def process_images(image_list):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Precomputing hashes...\")\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        chunk_size = max(1, len(image_list) // os.cpu_count())\n",
    "        chunks = [image_list[i:i + chunk_size] for i in range(0, len(image_list), chunk_size)]\n",
    "        results = list(tqdm(executor.map(precompute_hashes, chunks), total=len(chunks), desc=\"Computing hashes\"))\n",
    "\n",
    "    image_hashes = {k: v for d in results for k, v in d.items()}\n",
    "\n",
    "    print(\"Finding similar pairs...\")\n",
    "    similar_pairs = find_similar_pairs(image_hashes)\n",
    "\n",
    "    similar_pairs.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    print(\"\\nSimilar pairs above 0.85 threshold:\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i, (similarity, (img1_name, img2_name)) in enumerate(similar_pairs):\n",
    "            print(f\"\\nPair {i+1} with similarity: {similarity:.4f}\")\n",
    "            f.write(f\"Pair {i+1} with similarity: {similarity:.4f}\\n\")\n",
    "\n",
    "            img1_path = os.path.join(output_folder, img1_name)\n",
    "            img2_path = os.path.join(output_folder, img2_name)\n",
    "\n",
    "            print(\"Image 1:\")\n",
    "            print(f\"Filename: {img1_name}\")\n",
    "            f.write(f\"Image 1: {img1_name}\\n\")\n",
    "            display_image_with_filename(img1_path)\n",
    "\n",
    "            print(\"\\nImage 2:\")\n",
    "            print(f\"Filename: {img2_name}\")\n",
    "            f.write(f\"Image 2: {img2_name}\\n\\n\")\n",
    "            display_image_with_filename(img2_path)\n",
    "\n",
    "    print(f\"\\nTotal similar pairs found: {len(similar_pairs)}\")\n",
    "    print(f\"Results have been saved to {output_file}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Comparison complete! Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Processing all images in the folder...\")\n",
    "    all_images = os.listdir(output_folder)\n",
    "    process_images(all_images)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMeDTfs8T_FC"
   },
   "source": [
    "## 3. Remove duplicate images from folder images\n",
    "\n",
    "Find the lowest similarity where the images are real duplicaties and adjust code below. The images with the longest name are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI4xL-2XSgjC"
   },
   "outputs": [],
   "source": [
    "pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC_QZlMOKpUf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "# Output folder path (change as needed)\n",
    "output_folder = '/content/images'\n",
    "\n",
    "def compute_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"L\").resize((8, 8), Image.LANCZOS)  # Resize for hash computation\n",
    "            return str(imagehash.phash(img))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_and_remove_duplicates(folder):\n",
    "    hashes = {}\n",
    "\n",
    "    # Compute hashes for all images\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            img_hash = compute_image_hash(file_path)\n",
    "            if img_hash:\n",
    "                if img_hash in hashes:\n",
    "                    hashes[img_hash].append(filename)\n",
    "                else:\n",
    "                    hashes[img_hash] = [filename]\n",
    "\n",
    "    # Identify duplicates with high similarity and remove the one with the longest name\n",
    "    for img_hash, filenames in hashes.items():\n",
    "        if len(filenames) > 0.9922:  # More than one image with the same hash ---===+++ADJUST+++===---\n",
    "            # Check for similarity using Hamming distance\n",
    "            for i in range(len(filenames)):\n",
    "                for j in range(i + 1, len(filenames)):\n",
    "                    hash1 = imagehash.hex_to_hash(img_hash)\n",
    "                    hash2 = imagehash.hex_to_hash(compute_image_hash(os.path.join(folder, filenames[j])))\n",
    "                    similarity = 1 - (hash1 - hash2) / len(hash1.hash) ** 2\n",
    "\n",
    "                    if similarity >= 0.999:  # Only consider very similar images\n",
    "                        # Find the image with the longest name\n",
    "                        longest_name_image = max([filenames[i], filenames[j]], key=len)\n",
    "                        print(f\"Removing duplicate image: {longest_name_image}\")\n",
    "                        # Remove the image with the longest name from the folder\n",
    "                        os.remove(os.path.join(folder, longest_name_image))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_remove_duplicates(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_y3XNM6SY6Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "# Output folder path (change as needed)\n",
    "output_folder = '/content/images'\n",
    "\n",
    "def compute_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"L\").resize((8, 8), Image.LANCZOS)  # Resize for hash computation\n",
    "            return str(imagehash.phash(img))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_and_remove_duplicates(folder):\n",
    "    hashes = {}\n",
    "    total_removed = 0  # Initialize counter for removed images\n",
    "\n",
    "    # Compute hashes for all images\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            img_hash = compute_image_hash(file_path)\n",
    "            if img_hash:\n",
    "                if img_hash in hashes:\n",
    "                    hashes[img_hash].append(filename)\n",
    "                else:\n",
    "                    hashes[img_hash] = [filename]\n",
    "\n",
    "    # Identify duplicates with high similarity and remove the one with the longest name\n",
    "    for img_hash, filenames in hashes.items():\n",
    "        if len(filenames) > 1:  # More than one image with the same hash\n",
    "            for i in range(len(filenames)):\n",
    "                for j in range(i + 1, len(filenames)):\n",
    "                    hash1 = imagehash.hex_to_hash(img_hash)\n",
    "                    hash2 = imagehash.hex_to_hash(compute_image_hash(os.path.join(folder, filenames[j])))\n",
    "                    similarity = 1 - (hash1 - hash2) / len(hash1.hash) ** 2\n",
    "\n",
    "                    if similarity >= 0.9922:  # ADJUST THE SIMILARITY\n",
    "                        # Find the image with the longest name\n",
    "                        longest_name_image = max([filenames[i], filenames[j]], key=len)\n",
    "                        print(f\"Removing duplicate image: {longest_name_image}\")\n",
    "                        # Remove the image with the longest name from the folder\n",
    "                        os.remove(os.path.join(folder, longest_name_image))\n",
    "                        total_removed += 1  # Increment the counter\n",
    "\n",
    "    print(f\"\\nTotal number of duplicate images removed: {total_removed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_remove_duplicates(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZeWA1IQVdsE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the folder containing images and the name of the zip file\n",
    "output_folder = '/content/images'\n",
    "zip_file_name = '/content/images_noduplicates.zip'  # Specify the path for the zip file\n",
    "\n",
    "def zip_images(folder, zip_file):\n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        # Add all images in the folder to the zip file\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                zipf.write(file_path, arcname=filename)\n",
    "    print(f\"Created zip file: {zip_file}\")\n",
    "\n",
    "# Call the function to zip images\n",
    "zip_images(output_folder, zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEW6douPaTTf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP84ORX1nIHPLvYMAQXAxbV",
   "provenance": [
    {
     "file_id": "1BAgtWNfrK_pkPIuGEif8lv4ENAuhip4i",
     "timestamp": 1732731439288
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
